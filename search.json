[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Side Quest at Build Together Hackathon: jina-clip-v1 and LanceDB Cloud\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\nJun 8, 2024\n\n\nGabriel Chua\n\n\n\n\n\n\n\n\n\n\n\n\nNotes for “Chameleon: Mixed-Modal Early-Fusion Foundation Models”\n\n\n\n\n\n\npersonal notes\n\n\nllm\n\n\n\n\n\n\n\n\n\nJun 5, 2024\n\n\nGabriel Chua\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/jina-clip-lancedb-cloud/index.html",
    "href": "posts/jina-clip-lancedb-cloud/index.html",
    "title": "Side Quest at Build Together Hackathon: jina-clip-v1 and LanceDB Cloud",
    "section": "",
    "text": "Jina AI released their new multimodal embedding model - jina-clip-v1 this week. It’s under an Apache 2.0 license and it can be downloaded/used via HuggingFace. You can find out more details about the model from their blog and the technical report.\nAt the same time, I’m co-organising the Build Together Hackathon @ Singapore this weekend, and Ivan who was speaking managed to get the hackathon participants access to LanceDB Cloud, which I understand to be under technical preview. I last tried LanceDB at a hackathon, and thought trying the two tools would make a fun side quest today.\nThe objective of this blog post is to provide a simple code introduction and I highlight some of my learnings (e.g. how the embedding API does not automatically resize the image, using pydantic to define the db schema). The code below can certainly be further optimised with the batching of the API calls and data loading.\n\n\n\nMandatory “a picture paints a thousand words” photo for a blog about image-type models. Photo is by [Daian Gan](https://www.pexels.com/photo/shallow-focus-photography-of-paintbrush-102127/\n\n\n\nStep 1: Connecting to LanceDB Cloud\nLet’s begin by defining some constants related to LanceDB Cloud - the vector database’s URI and the API key. We can then connect to the database.\nimport os\nimport lancedb\n\n# Load API keys from environment variables\nLANCEDB_API_KEY = os.getenv(\"LANCEDB_API_KEY\")\nLANCEDB_URI = os.getenv(\"LANCEDB_URI\")\n\n# Connect to the database\ndb = lancedb.connect(\n        uri=LANCEDB_URI,\n        api_key=LANCEDB_API_KEY,\n        region=\"us-east-1\"\n    )\nThe lancedb.connect line is the main difference between using LanceDB locally and via their cloud managed service - so transitioning between the two is that easy. If you’re using it locally, the URI will be the filepath of your choice, and you won’t need the api_key or region argument.. Also, do note that we’re using LanceDB’s synchronous API throughout this blog.\nLOCAL_FILE_PATH = \"xxx\"\n\ndb = lancedb.connect(\n        uri=LOCAL_FILE_PATH,\n    )\n\n\nStep 2: Helper functions to get multimodal embeddings\nIn this example, we’re using Jina AI’s API service. As part of the free trial, you have free credits to process up to 1M tokens.\nLet’s go into the code. We begin by loading the API key too.\nJINA_API_KEY = os.getenv(\"JINA_API_KEY\")\nAccording to the docs, the image sent to the API can either be a URL or bytes. Hence, we write the following helper code below to encode the image to base64.\n\n\n\nScreenshot of Jina API Docs\n\n\nimport base64\nfrom io import BytesIO\n\n# Encode an image to base 64\ndef image_to_base64(image):\n    buffered = BytesIO()\n    image.save(buffered, format=\"PNG\")\n    return base64.b64encode(buffered.getvalue()).decode('utf-8')\nWhat about the size of the image? According to their blog, every 224x224 pixel tile in the image is 1,000 tokens. I didn’t know that initially and assumed that the image would be automatically resized to a suitable size in the backend. I accidentally sent in a ~1MB image, and used almost half of my free credits.\nFor how tokens are counted for images larger than 224x224, let’s refer to the example from their blog.\n\nFor an image with dimensions 750x500 pixels:\n\nThe image is divided into 224x224 pixel tiles.\nTo calculate the number of tiles, take the width in pixels and divide by 224, then round up to the nearest integer. 750/224 ≈ 3.35 → 4\nRepeat for the height in pixels: 500/224 ≈ 2.23 → 3\nThe total number of tiles required in this example is: 4 (horizontal) x 3 (vertical) = 12 tiles\nThe cost will be 12 x 1,000 = 12,000 tokens\n\n\nHence, I’ve written a simple helper function to resize the image to the lowest tile resolution the model takes in - which is 224x224\nfrom PIL import Image\n\n# Resize image to 214x214\ndef resize_image(image_file_path, size=(224, 224)):\n    \"\"\" Resize image to fit within the given size (224, 224) \"\"\"\n    with Image.open(image_file_path) as img:\n        img.thumbnail(size, Image.LANCZOS)\n        return img\nWith these two helper functions, we can now write our main embedding function which does a POST request.\nimport json\nimport requests\n\ndef get_embeddings(image_file_path):\n    headers = {\n        'Content-Type': 'application/json',\n        'Authorization': f'Bearer {JINA_API_KEY}'\n    }\n\n    resized_image = resize_image(image_file_path)\n\n    base64_image = image_to_base64(resized_image)\n    \n    data = {\n        'input': [{\"bytes\": base64_image}],\n        'model': 'jina-clip-v1',\n        'encoding_type': 'float'\n        }\n\n    response = requests.post(JINA_ENDPOINT,\n                             headers=headers,\n                             json=data)\n    \n    results = json.loads(response.text)\n\n    return results[\"data\"][0][\"embedding\"]\n\n\nStep 3: Loading the embeddings into the vector database\nWhat’s nice about LanceDB is that we can use Pydantic to programmatically define how we want to store our data. For the Vector type, we set it to the length 768 and that is the resulting dimensionality of the embedding vectors from jina-clip-v1.\nfrom lancedb.pydantic import Vector, LanceModel\n\n# Create a schema for the table\nclass Content(LanceModel):\n    file_id: int\n    file_name: str\n    vector: Vector(768)\n\n# Create a table called `demo` based on the above schema\ntbl = db.create_table(\"demo\", schema=Content)\n\n\nStep 4: Loading the embeddings into the vector database\nNow that we can generate our embeddings and load it into the vector database.\nFor the code below, assume list_of_files is a list of file paths of the images we want to embed.\n# Loop through each file in the list\nfor index, file_name in enumerate(LIST_OF_FILES):\n\n    # Create the embeddings\n    image_embedding = get_embeddings(file_name)\n\n    # Store this as a list of dictionaries to load into the vector database\n    img_data_to_add = [\n        {\n            \"file_id\": index\n            \"vector\": image_embedding,\n            \"file_name\": f\"{file_name}\",\n        }\n    ]\n\n    # Add to the db\n    tbl.add(img_data_to_add)\n\n\nFull code\n# Load packages\nimport base64\nimport json\nimport os\nfrom io import BytesIO\n\nimport lancedb\nimport requests\nfrom lancedb.pydantic import Vector, LanceModel\nfrom PIL import Image\n\n# Load secrets from environment variables\nLANCEDB_API_KEY = os.getenv(\"LANCEDB_API_KEY\")\nLANCEDB_URI = os.getenv(\"LANCEDB_URI\")\nJINA_API_KEY = os.getenv(\"JINA_API_KEY\")\nLIST_OF_FILES = [\"path/to/file1.png\", \"path/to/file2.png\"] # list of file names\n\n# Encode an image to base 64\ndef image_to_base64(image):\n    buffered = BytesIO()\n    image.save(buffered, format=\"PNG\")\n    return base64.b64encode(buffered.getvalue()).decode('utf-8')\n\n# Resize image to 214x214\ndef resize_image(image_file_path, size=(214, 214)):\n    \"\"\" Resize image to fit within the given size (214, 214) \"\"\"\n    with Image.open(image_file_path) as img:\n        img.thumbnail(size, Image.LANCZOS)\n        return img\n\n# Get embeddings\ndef get_embeddings(image_file_path):\n    headers = {\n        'Content-Type': 'application/json',\n        'Authorization': f'Bearer {JINA_API_KEY}'\n    }\n\n    resized_image = resize_image(image_file_path)\n\n    base64_image = image_to_base64(resized_image)\n    \n    data = {\n        'input': [{\"bytes\": base64_image}],\n        'model': 'jina-clip-v1',\n        'encoding_type': 'float'\n        }\n\n    response = requests.post(\"https://api.jina.ai/v1/embeddings\",\n                             headers=headers,\n                             json=data)\n    \n    results = json.loads(response.text)\n\n    return results[\"data\"][0][\"embedding\"]\n\n\n# Connect to the database\ndb = lancedb.connect(\n        uri=LANCEDB_URI,\n        api_key=LANCEDB_API_KEY,\n        region=\"us-east-1\"\n    )\n\n\n# Create a schema for the table\nclass Content(LanceModel):\n    file_id: int\n    file_name: str\n    vector: Vector(768)\n\n# Create a table called `demo` based on the above schema\ntbl = db.create_table(\"demo\", schema=Content)\n\n# Loop through list of file names to generate embedding and add to db\nfor index, file_name in enumerate(LIST_OF_FILES):\n    \n    # Create the embeddings\n    image_embedding = get_embeddings(file_name)\n\n    # Store this as a list of dictionaries to load into the vector database\n    img_data_to_add = [\n        {\n            \"file_id\": index,\n            \"vector\": image_embedding,\n            \"file_name\": f\"{file_name}\",\n        }\n    ]\n\n    # Add to the db\n    tbl.add(img_data_to_add)"
  },
  {
    "objectID": "posts/chameleon-meta/index.html",
    "href": "posts/chameleon-meta/index.html",
    "title": "Notes for “Chameleon: Mixed-Modal Early-Fusion Foundation Models”",
    "section": "",
    "text": "Here is the link to the original paper. These notes were prepared for my LLM Asia Paper Club sharing. Any feedback or areas for improvement would be most appreciated at cyzgab[at]gmail.com.\n\n\nKey Points\n\nEnd-to-end multimodal tokens (i.e. no modality-specific encoder or decoder)\nNovelties in architectural innovations and training techniques to address computational challenges:\n\nquery-key normalization\nrevised placement of layer norms\n\nPre-training these models require large datasets and computation.\n\nDataset of 4.4T (2.9T is text only, 1.5T is text-to-image, and 400B is text-and-image-interleaved)\n7B and 34B trained on 856K and 4.2M GPU hours respectively\n\nPerformance:\n\nOn Visual Q&A, outperforms Flamingo, Llava-1.5\nOn text-only benchmarks, still competitive with Mixtral 8x7B and Gemini-Pro\nOn human pairwise comparisons, beats Gemini-Pro and GPT-4V\n\n\n\n\n\n\nImage 1: Conceptual summary of multimodal training and generation from the paper\n\n\n\n\n\n\nImage 2: Example generation from the paper\n\n\n\n\n\nLate vs Early Fusion\n\nA useful reference for me was this literature review by Wadekar et.al\n\nLate fusion Done at the internal layers of the model (e.g. OpenFlamingo, LLaMA-Adapter-V2)\n\n\n\n\nImage 3: Simplified architectural summary for late fusion - taken from Wadekar et.al\n\n\n\nEarly fusion Done at the input stage (e.g. LLaVA, Unified-IO-2, Chameleon, Gemini)\n\n\n\n\nImage 4: Simplified architectural summary for non-tokenised early fusion - taken from Wadekar et.al\n\n\n\n\n\n\nImage 5: Simplified architectural summary for tokenised early fusion - taken from Wadekar et.al\n\n\n\n\n\nTokeniser\n\nFor images: trained a new image tokenizer based on Gafni et.al (2022) which encodes a 512 × 512 image into 1024 discrete tokens from a codebook of size 8192\n\n\nBased on OAI’s pricing page (as of 5 June 2024), one image is ~170 tokens in GPT-4o.\n\n\nFor text: BPE tokenizer over a subset of training data, with a vocabulary size of 65K\n\n\n\nEnsuring Stability of Pre-Training\n\n“We found that the standard LLaMa architecture showed complex divergences due to slow norm growth in the mid-to-late stages of training. We narrowed down the cause of the divergence to the softmax operation being problematic when training with multiple modalities of significantly varying entropy due to the translation invariant property of softmax (i.e., softmax(z) = softmax(z+c)). Because we share all weights of the model across modalities, each modality will try to “compete” with the other by increasing its norms slightly; while not problematic at the beginning of training, it manifests in divergences once we get outside the effective representation range of bf16… In a unimodal setting, this problem has also been named the logit drift problem.” (Page 6)\n\n\nQuery-key normalisation: applying layer norm to the query and key vectors within the attention.\nRevised placement of layer norms for 34B model\nIntroducing z-loss regularisation\n\n\n\n\n\nImage 6: Training plots\n\n\n\nDropout was initially introduced after the attention and feed forward layer for the 7B model, though subsequently found to be not necessary. For the 34B model, dropout was not sufficient (nor necessary).\n\n\nSummary of Pre-Training\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nParams\nContext Length\nGQA\nTokens\nLR\nEpochs\nDropout\nZloss\nQknorm\n\n\n\n\nLLMa-1\n7B\n2k\n✗\n1.0T\n3.0 × 10^-4\n1.0\n0.0\n0.0\n✗\n\n\n\n33B\n2k\n✗\n1.4T\n1.5 × 10^-4\n1.0\n0.0\n0.0\n✗\n\n\nLLMa-2\n7B\n4k\n✗\n2.0T\n3.0 × 10^-4\n1.0\n0.0\n0.0\n✗\n\n\n\n34B\n4k\n✓\n2.0T\n1.5 × 10^-4\n1.0\n0.0\n0.0\n✗\n\n\nChameleon\n7B\n4k\n✗\n4.4T\n1.0 × 10^-4\n2.1\n0.1\n10^-5\n✓\n\n\n\n34B\n4k\n✓\n4.4T\n1.0 × 10^-4\n2.1\n0.0\n10^-5\n✓\n\n\n\nTaken from Table 1 of the paper\n\n\nChallenges Associated with Inference\n\nWhen decoding, we need to check whether it is a text or image token\nMasking tokens from other modalities when exclusively generating for a particular modality (e.g. no text tokens when doing image-only generation)\nToken-based image generation is a fixed-sized block\n\n\n\nSFT/Alignment\nSupervised fine-tuning dataset covered the following categories:\n\ntext\ncode\nvisual chat\nimage generation\ninterleaved text/image generation\nsafety (e.g. “I can’t help with that”)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hello 👋",
    "section": "",
    "text": "I am a Data Scientist at GovTech where my current work focuses on MLOps and prototyping LLM applications for labour market applications.\nPreviously, I served as a policy analyst at the Ministry of Health reviewing primary care financing policies (e.g. subsidies, capitation funding for population health) and data analytics for COVID-19. My proudest achievements include contributing to the data pipelines for the nightly COVID-19 press releases, and co-authoring a White Paper that was debated in Parliament.\nI gravitate towards new ideas and am curious about the intersection of different disciplines. With a background in the social sciences, my career evolved towards applied machine learning. I hold a BSc in Economics from LSE and a Master’s in Business Analytics from MIT.\nOutside of work, I enjoy pilates, a good beer, and hiking."
  }
]