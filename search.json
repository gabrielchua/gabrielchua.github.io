[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "A Simple Off-Topic Guardrail\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\nAug 17, 2024\n\n\nGabriel Chua\n\n\n\n\n\n\n\n\n\n\n\n\nNavigating the limits of LLM\n\n\n\n\n\n\nllm\n\n\ntalk\n\n\n\n\n\n\n\n\n\nAug 10, 2024\n\n\nGabriel Chua\n\n\n\n\n\n\n\n\n\n\n\n\nDeploying HuggingFace models on AWS SageMaker with 2 commands\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\nAug 10, 2024\n\n\nGabriel Chua\n\n\n\n\n\n\n\n\n\n\n\n\nFine-tuning TinyLlama with Axolotl and JarvisLab\n\n\n\n\n\n\nllm\n\n\ncode\n\n\n\nFine-tuning TinyLlama to generate David Attenborough style narration - using Axolotl, JarvisLab, and OpenAI’s Batch API\n\n\n\n\n\nJun 16, 2024\n\n\nGabriel Chua\n\n\n\n\n\n\n\n\n\n\n\n\nWorking with LLMs: A Data Analyst’s Guide\n\n\n\n\n\n\nllm\n\n\ntalk\n\n\n\n\n\n\n\n\n\nJun 14, 2024\n\n\nGabriel Chua\n\n\n\n\n\n\n\n\n\n\n\n\nSide Quest at Build Together Hackathon: jina-clip-v1 and LanceDB Cloud\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\nJun 8, 2024\n\n\nGabriel Chua\n\n\n\n\n\n\n\n\n\n\n\n\nNotes for “Chameleon: Mixed-Modal Early-Fusion Foundation Models”\n\n\n\n\n\n\npersonal notes\n\n\nllm\n\n\n\n\n\n\n\n\n\nJun 5, 2024\n\n\nGabriel Chua\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/off-topic-guardrail/index.html",
    "href": "posts/off-topic-guardrail/index.html",
    "title": "A Simple Off-Topic Guardrail",
    "section": "",
    "text": "In this post, we’ll explore how to build a straightforward off-topic detector using embeddings and Approximate Nearest Neighbors (KNN).\nAn off-topic guardrail is essential for filtering out queries that don’t align with the intended purpose of an LLM application. For instance, in an LLM application focused on car sales, we wouldn’t want it to start discussing Python code.\nSince this guardrail will be integrated into the main LLM application, it must be fast and simple to implement, ensuring it doesn’t introduce significant latency or complexity.\nThe key lies in using embeddings to capture the semantic meaning of the input text. By converting both the input query and a set of predefined on-topic and off-topic texts into embeddings, we can represent them in a high-dimensional space and analyze their relative positions. In this example, we use OpenAI’s text-embedding-3-small, though any reasonably performant embedding model can be substituted.\nThe goal is to determine whether the input query closely relates to the topics of interest or if it ventures into “off-topic” territory.\nThis process essentially becomes a binary classification problem, where we leverage K-Nearest Neighbors (KNN) to classify the input text. Using the Annoy library, we can efficiently find the closest embeddings in our dataset. By analyzing the nearest neighbors and their associated labels, we calculate weighted probabilities to determine whether the input text is likely on-topic or off-topic.\n\n\nimport numpy as np\nimport argparse\nfrom openai import OpenAI\nfrom annoy import AnnoyIndex\n\n# Categories for on_topic and off_topic texts\nON_TOPIC_TEXTS = [\n    \"What is the capital of China?\",\n    \"What is the currency of UK?\",\n    \"Timezone for New York?\",\n    \"Which country has the largest population?\",\n    \"What is the largest island in the world?\",\n]\n\nOFF_TOPIC_TEXTS = [\n    \"Write a python code\",\n    \"Explain the meaning of life\",\n    \"Why is the sky blue?\",\n]\n\n# Parse command-line arguments\nparser = argparse.ArgumentParser(description=\"Classify whether text is off-topic.\")\nparser.add_argument(\"input_text\", type=str, help=\"Input text to classify.\")\nargs = parser.parse_args()\n\n# Initialize OpenAI client\nclient = OpenAI()\n\n# Input text from CLI\ninput_text = args.input_text\n\n# Combine all texts for embedding\nall_texts = [input_text] + ON_TOPIC_TEXTS + OFF_TOPIC_TEXTS\n\n# Get embeddings\nresponse = client.embeddings.create(\n    model=\"text-embedding-3-small\",\n    input=all_texts,\n)\n\n# Extract embeddings\ninput_embedding = response.data[0].embedding\non_topic_embeddings = [response.data[i].embedding for i in range(1, len(ON_TOPIC_TEXTS) + 1)]\noff_topic_embeddings = [response.data[i].embedding for i in range(len(ON_TOPIC_TEXTS) + 1, len(all_texts))]\n\n# Prepare data for Annoy\nembeddings = on_topic_embeddings + off_topic_embeddings\nlabels = [\"on_topic\"] * len(on_topic_embeddings) + [\"off_topic\"] * len(off_topic_embeddings)\n\n# Initialize Annoy index\nf = len(input_embedding)\nannoy_index = AnnoyIndex(f, 'angular')  # Using 'angular' for cosine similarity\n\n# Add items to Annoy index\nfor i, emb in enumerate(embeddings):\n    annoy_index.add_item(i, emb)\n\n# Build Annoy index\nannoy_index.build(10)  # Number of trees can be adjusted\n\n# Define a function to calculate weighted probabilities\ndef get_weighted_probabilities(annoy_index, labels, query_vector, k=3):\n    # Get k nearest neighbors with distances\n    neighbors, distances = annoy_index.get_nns_by_vector(query_vector, k, include_distances=True)\n    \n    # Inverse distance weights (adding small epsilon to avoid division by zero)\n    weights = 1 / (np.array(distances) + 1e-8)\n    \n    # Calculate weighted probabilities\n    class_weights = {}\n    for i, label in enumerate([labels[n] for n in neighbors]):\n        if label in class_weights:\n            class_weights[label] += weights[i]\n        else:\n            class_weights[label] = weights[i]\n    \n    total_weight = sum(class_weights.values())\n    probabilities = {label: weight / total_weight for label, weight in class_weights.items()}\n    return probabilities\n\n# Classify input text and calculate weighted probability\nprobabilities = get_weighted_probabilities(annoy_index, labels, input_embedding, k=3)\nclassification = max(probabilities, key=probabilities.get)\n\n# Output classification and probability\nprint(f\"Input text classification: {classification}\")\nprint(f\"Probability off-topic: {probabilities.get('off_topic', 0)}\")"
  },
  {
    "objectID": "posts/off-topic-guardrail/index.html#code-implementation",
    "href": "posts/off-topic-guardrail/index.html#code-implementation",
    "title": "A Simple Off-Topic Guardrail",
    "section": "",
    "text": "import numpy as np\nimport argparse\nfrom openai import OpenAI\nfrom annoy import AnnoyIndex\n\n# Categories for on_topic and off_topic texts\nON_TOPIC_TEXTS = [\n    \"What is the capital of China?\",\n    \"What is the currency of UK?\",\n    \"Timezone for New York?\",\n    \"Which country has the largest population?\",\n    \"What is the largest island in the world?\",\n]\n\nOFF_TOPIC_TEXTS = [\n    \"Write a python code\",\n    \"Explain the meaning of life\",\n    \"Why is the sky blue?\",\n]\n\n# Parse command-line arguments\nparser = argparse.ArgumentParser(description=\"Classify whether text is off-topic.\")\nparser.add_argument(\"input_text\", type=str, help=\"Input text to classify.\")\nargs = parser.parse_args()\n\n# Initialize OpenAI client\nclient = OpenAI()\n\n# Input text from CLI\ninput_text = args.input_text\n\n# Combine all texts for embedding\nall_texts = [input_text] + ON_TOPIC_TEXTS + OFF_TOPIC_TEXTS\n\n# Get embeddings\nresponse = client.embeddings.create(\n    model=\"text-embedding-3-small\",\n    input=all_texts,\n)\n\n# Extract embeddings\ninput_embedding = response.data[0].embedding\non_topic_embeddings = [response.data[i].embedding for i in range(1, len(ON_TOPIC_TEXTS) + 1)]\noff_topic_embeddings = [response.data[i].embedding for i in range(len(ON_TOPIC_TEXTS) + 1, len(all_texts))]\n\n# Prepare data for Annoy\nembeddings = on_topic_embeddings + off_topic_embeddings\nlabels = [\"on_topic\"] * len(on_topic_embeddings) + [\"off_topic\"] * len(off_topic_embeddings)\n\n# Initialize Annoy index\nf = len(input_embedding)\nannoy_index = AnnoyIndex(f, 'angular')  # Using 'angular' for cosine similarity\n\n# Add items to Annoy index\nfor i, emb in enumerate(embeddings):\n    annoy_index.add_item(i, emb)\n\n# Build Annoy index\nannoy_index.build(10)  # Number of trees can be adjusted\n\n# Define a function to calculate weighted probabilities\ndef get_weighted_probabilities(annoy_index, labels, query_vector, k=3):\n    # Get k nearest neighbors with distances\n    neighbors, distances = annoy_index.get_nns_by_vector(query_vector, k, include_distances=True)\n    \n    # Inverse distance weights (adding small epsilon to avoid division by zero)\n    weights = 1 / (np.array(distances) + 1e-8)\n    \n    # Calculate weighted probabilities\n    class_weights = {}\n    for i, label in enumerate([labels[n] for n in neighbors]):\n        if label in class_weights:\n            class_weights[label] += weights[i]\n        else:\n            class_weights[label] = weights[i]\n    \n    total_weight = sum(class_weights.values())\n    probabilities = {label: weight / total_weight for label, weight in class_weights.items()}\n    return probabilities\n\n# Classify input text and calculate weighted probability\nprobabilities = get_weighted_probabilities(annoy_index, labels, input_embedding, k=3)\nclassification = max(probabilities, key=probabilities.get)\n\n# Output classification and probability\nprint(f\"Input text classification: {classification}\")\nprint(f\"Probability off-topic: {probabilities.get('off_topic', 0)}\")"
  },
  {
    "objectID": "posts/deploying-hf-models-sagemaker/index.html",
    "href": "posts/deploying-hf-models-sagemaker/index.html",
    "title": "Deploying HuggingFace models on AWS SageMaker with 2 commands",
    "section": "",
    "text": "You can have a HuggingFace model up and running on SageMaker in just a few lines of code.\n===\nFirst, you need to import the necessary modules:\nfrom sagemaker import get_execution_role\nfrom sagemaker.huggingface.model import HuggingFaceModel\nSet up the necessary environment variables, including the model ID, instance type, and versions.\nENDPOINT_NAME = \"baai-bge-large-en-v1-5\"\n\nHF_ENV = {\n    'HF_MODEL_ID':'BAAI/bge-large-en-v1.5',\n    'HF_TASK':'feature-extraction'\n}\n\nINSTANCE_TYPE = \"ml.m5.xlarge\"\n\nTRANSFORMER_VER = \"4.26\"\n\nPY_VERSION = \"py39\"\n\nPYTORCH_VERSION = \"1.13\"\nCreate a HuggingFaceModel model with the specified configurations.\nHere we are using SageMaker’s built-in container images with specific versions of python, pytorch and transformers. A full list of available images can be found here.\nhuggingface_model = HuggingFaceModel(\n   env=HF_ENV,\n   role=get_execution_role(),\n   transformers_version=TRANSFORMER_VER,\n   pytorch_version=PYTORCH_VERSION,\n   py_version=PY_VERSION,\n)\nThen use the .deploy method.\npredictor = huggingface_model.deploy(\n    endpoint_name=ENDPOINT_NAME,\n    initial_instance_count=1,\n    instance_type=INSTANCE_TYPE\n)\nAnd that’s it! With just a few lines of code, your HuggingFace model is live on AWS SageMaker. It’s incredibly fast to get started and deploy."
  },
  {
    "objectID": "posts/chameleon-meta/index.html",
    "href": "posts/chameleon-meta/index.html",
    "title": "Notes for “Chameleon: Mixed-Modal Early-Fusion Foundation Models”",
    "section": "",
    "text": "Here is the link to the original paper. These notes were prepared for my LLM Asia Paper Club sharing. Any feedback or areas for improvement would be most appreciated at cyzgab[at]gmail.com.\n\n\nKey Points\n\nEnd-to-end multimodal tokens (i.e. no modality-specific encoder or decoder)\nNovelties in architectural innovations and training techniques to address computational challenges:\n\nquery-key normalization\nrevised placement of layer norms\n\nPre-training these models require large datasets and computation.\n\nDataset of 4.4T (2.9T is text only, 1.5T is text-to-image, and 400B is text-and-image-interleaved)\n7B and 34B trained on 856K and 4.2M GPU hours respectively\n\nPerformance:\n\nOn Visual Q&A, outperforms Flamingo, Llava-1.5\nOn text-only benchmarks, still competitive with Mixtral 8x7B and Gemini-Pro\nOn human pairwise comparisons, beats Gemini-Pro and GPT-4V\n\n\n\n\n\n\nImage 1: Conceptual summary of multimodal training and generation from the paper\n\n\n\n\n\n\nImage 2: Example generation from the paper\n\n\n\n\n\nLate vs Early Fusion\n\nA useful reference for me was this literature review by Wadekar et.al\n\nLate fusion Done at the internal layers of the model (e.g. OpenFlamingo, LLaMA-Adapter-V2)\n\n\n\n\nImage 3: Simplified architectural summary for late fusion - taken from Wadekar et.al\n\n\n\nEarly fusion Done at the input stage (e.g. LLaVA, Unified-IO-2, Chameleon, Gemini)\n\n\n\n\nImage 4: Simplified architectural summary for non-tokenised early fusion - taken from Wadekar et.al\n\n\n\n\n\n\nImage 5: Simplified architectural summary for tokenised early fusion - taken from Wadekar et.al\n\n\n\n\n\nTokeniser\n\nFor images: trained a new image tokenizer based on Gafni et.al (2022) which encodes a 512 × 512 image into 1024 discrete tokens from a codebook of size 8192\n\n\nBased on OAI’s pricing page (as of 5 June 2024), one image is ~170 tokens in GPT-4o.\n\n\nFor text: BPE tokenizer over a subset of training data, with a vocabulary size of 65K\n\n\n\nEnsuring Stability of Pre-Training\n\n“We found that the standard LLaMa architecture showed complex divergences due to slow norm growth in the mid-to-late stages of training. We narrowed down the cause of the divergence to the softmax operation being problematic when training with multiple modalities of significantly varying entropy due to the translation invariant property of softmax (i.e., softmax(z) = softmax(z+c)). Because we share all weights of the model across modalities, each modality will try to “compete” with the other by increasing its norms slightly; while not problematic at the beginning of training, it manifests in divergences once we get outside the effective representation range of bf16… In a unimodal setting, this problem has also been named the logit drift problem.” (Page 6)\n\n\nQuery-key normalisation: applying layer norm to the query and key vectors within the attention.\nRevised placement of layer norms for 34B model\nIntroducing z-loss regularisation\n\n\n\n\n\nImage 6: Training plots\n\n\n\nDropout was initially introduced after the attention and feed forward layer for the 7B model, though subsequently found to be not necessary. For the 34B model, dropout was not sufficient (nor necessary).\n\n\nSummary of Pre-Training\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nParams\nContext Length\nGQA\nTokens\nLR\nEpochs\nDropout\nZloss\nQknorm\n\n\n\n\nLLMa-1\n7B\n2k\n✗\n1.0T\n3.0 × 10^-4\n1.0\n0.0\n0.0\n✗\n\n\n\n33B\n2k\n✗\n1.4T\n1.5 × 10^-4\n1.0\n0.0\n0.0\n✗\n\n\nLLMa-2\n7B\n4k\n✗\n2.0T\n3.0 × 10^-4\n1.0\n0.0\n0.0\n✗\n\n\n\n34B\n4k\n✓\n2.0T\n1.5 × 10^-4\n1.0\n0.0\n0.0\n✗\n\n\nChameleon\n7B\n4k\n✗\n4.4T\n1.0 × 10^-4\n2.1\n0.1\n10^-5\n✓\n\n\n\n34B\n4k\n✓\n4.4T\n1.0 × 10^-4\n2.1\n0.0\n10^-5\n✓\n\n\n\nTaken from Table 1 of the paper\n\n\nChallenges Associated with Inference\n\nWhen decoding, we need to check whether it is a text or image token\nMasking tokens from other modalities when exclusively generating for a particular modality (e.g. no text tokens when doing image-only generation)\nToken-based image generation is a fixed-sized block\n\n\n\nSFT/Alignment\nSupervised fine-tuning dataset covered the following categories:\n\ntext\ncode\nvisual chat\nimage generation\ninterleaved text/image generation\nsafety (e.g. “I can’t help with that”)"
  },
  {
    "objectID": "posts/limits-of-llms-talk/index.html",
    "href": "posts/limits-of-llms-talk/index.html",
    "title": "Navigating the limits of LLM",
    "section": "",
    "text": "This blog post summarizes an online talk I gave for the Prompt Royale 2024.\n\n\n\nKey Points\n\nJagged Intelligence: As observed by Karpathy, large language models (LLMs) display a “jagged intelligence”—they can execute highly impressive tasks while simultaneously struggling with simple, sometimes trivial problems.\nThe ability of LLMs to process information within absolute context lengths is improving, such as overcoming the “lost in the middle” issue. However, it remains crucial to optimize context usage by focusing on one call or chat per task, rather than overwhelming the model with excessive context.\nThe auto-regressive nature of LLMs often contributes to “hallucinations.” Once the model generates an answer, it tends to “force fit” subsequent reasoning to align with that answer. Certain prompting techniques, like chain-of-thought and reflection, can help mitigate these issues, though results may vary.\nAre these models genuinely reasoning, or are they merely memorizing a vast array of brain teasers and interpolating from them? This remains a critical area of inquiry.\nWhen it comes to math, the model’s tokenizer plays a significant role. Leveraging external tools, such as the AIMO NuminaMath model, can enhance accuracy.\nUltimately, some limitations of LLMs are deeply tied to their architecture—issues like tokenization and reasoning challenges. However, others, such as context length and reasoning capabilities, may improve as models continue to scale and computing power increases. With models becoming significantly cheaper, faster, and more capable every 6-12 months, it’s important to build with the future in mind. Don’t let the limitations of today’s models constrain your vision—anticipate and prepare for the advancements on the horizon.\n\n\n\nSlides"
  },
  {
    "objectID": "posts/llm-for-data-analysis/index.html",
    "href": "posts/llm-for-data-analysis/index.html",
    "title": "Working with LLMs: A Data Analyst’s Guide",
    "section": "",
    "text": "This blog post summarizes a talk I gave at GovTech’s Data Science and AI Connect, a regular gathering of practitioners and enthusiasts across Singapore’s public service. Below are some key points and reflections from my presentation.\n\n\n\nKey Points\n\nLLMs as Calculators for Open-Ended Tasks:\n\nInstead of viewing LLMs as advanced autocompletes, think of them as calculators for open-ended tasks.\nJust as you’d use a calculator for complex arithmetic, LLMs can handle tasks like extracting last names from a list or classifying restaurant reviews as positive or negative.\n\nUnderstanding the Tokenizer:\n\nFor technical audiences, I emphasized the importance of understanding the tokenizer and its quirks. As Andrej Karpathy notes, many limitations of LLMs stem from the tokenizer.\n\nDevelopments in Data Analysis:\n\nI categorized advancements into LLMs that write code and those that write and execute code. The latter can rewrite code based on results or errors, making them more agentic.\nText2SQL is popular with models like Defog.ai and Pandas AI.\nI demoed ChatGPT’s code interpreter, a feature I believe is underappreciated. While not perfect, it has great potential to empower domain experts and speed up insights and decision-making.\n\nLLMs in Classical Machine Learning:\n\nLLMs significantly reduce the time needed to deploy an initial prototype. Ship the application quickly and improve it over time with collected data.\n\nPractical Tips:\n\nAdjust max_tokens and logit_bias parameters to use LLMs as zero/few-shot classifiers that return confidence scores.\nUse LLMs to generate additional features (columns) and examples (rows).\nEmbedding models, though not technically LLMs, can be used out-of-the-box for initial prototypes in text classification tasks.\n\nResources for Getting Started:\n\nI concluded with resources on how to begin utilizing LLMs (Slides 23 to 26).\n\n\n\n\nSlides"
  },
  {
    "objectID": "posts/jina-clip-lancedb-cloud/index.html",
    "href": "posts/jina-clip-lancedb-cloud/index.html",
    "title": "Side Quest at Build Together Hackathon: jina-clip-v1 and LanceDB Cloud",
    "section": "",
    "text": "Jina AI released their new multimodal embedding model - jina-clip-v1 this week. It’s under an Apache 2.0 license and it can be downloaded/used via HuggingFace. You can find out more details about the model from their blog and the technical report.\nAt the same time, I’m co-organising the Build Together Hackathon @ Singapore this weekend, and Ivan who was speaking managed to get the hackathon participants access to LanceDB Cloud, which I understand to be under technical preview. I last tried LanceDB at a hackathon, and thought trying the two tools would make a fun side quest today.\nThe objective of this blog post is to provide a simple code introduction and I highlight some of my learnings (e.g. how the embedding API does not automatically resize the image, using pydantic to define the db schema). The code below can certainly be further optimised with the batching of the API calls and data loading.\n\n\n\nMandatory “a picture paints a thousand words” photo for a blog about image-type models. Photo is by [Daian Gan](https://www.pexels.com/photo/shallow-focus-photography-of-paintbrush-102127/\n\n\n\nStep 1: Connecting to LanceDB Cloud\nLet’s begin by defining some constants related to LanceDB Cloud - the vector database’s URI and the API key. We can then connect to the database.\nimport os\nimport lancedb\n\n# Load API keys from environment variables\nLANCEDB_API_KEY = os.getenv(\"LANCEDB_API_KEY\")\nLANCEDB_URI = os.getenv(\"LANCEDB_URI\")\n\n# Connect to the database\ndb = lancedb.connect(\n        uri=LANCEDB_URI,\n        api_key=LANCEDB_API_KEY,\n        region=\"us-east-1\"\n    )\nThe lancedb.connect line is the main difference between using LanceDB locally and via their cloud managed service - so transitioning between the two is that easy. If you’re using it locally, the URI will be the filepath of your choice, and you won’t need the api_key or region argument.. Also, do note that we’re using LanceDB’s synchronous API throughout this blog.\nLOCAL_FILE_PATH = \"xxx\"\n\ndb = lancedb.connect(\n        uri=LOCAL_FILE_PATH,\n    )\n\n\nStep 2: Helper functions to get multimodal embeddings\nIn this example, we’re using Jina AI’s API service. As part of the free trial, you have free credits to process up to 1M tokens.\nLet’s go into the code. We begin by loading the API key too.\nJINA_API_KEY = os.getenv(\"JINA_API_KEY\")\nAccording to the docs, the image sent to the API can either be a URL or bytes. Hence, we write the following helper code below to encode the image to base64.\n\n\n\nScreenshot of Jina API Docs\n\n\nimport base64\nfrom io import BytesIO\n\n# Encode an image to base 64\ndef image_to_base64(image):\n    buffered = BytesIO()\n    image.save(buffered, format=\"PNG\")\n    return base64.b64encode(buffered.getvalue()).decode('utf-8')\nWhat about the size of the image? According to their blog, every 224x224 pixel tile in the image is 1,000 tokens. I didn’t know that initially and assumed that the image would be automatically resized to a suitable size in the backend. I accidentally sent in a ~1MB image, and used almost half of my free credits.\nFor how tokens are counted for images larger than 224x224, let’s refer to the example from their blog.\n\nFor an image with dimensions 750x500 pixels:\n\nThe image is divided into 224x224 pixel tiles.\nTo calculate the number of tiles, take the width in pixels and divide by 224, then round up to the nearest integer. 750/224 ≈ 3.35 → 4\nRepeat for the height in pixels: 500/224 ≈ 2.23 → 3\nThe total number of tiles required in this example is: 4 (horizontal) x 3 (vertical) = 12 tiles\nThe cost will be 12 x 1,000 = 12,000 tokens\n\n\nHence, I’ve written a simple helper function to resize the image to the lowest tile resolution the model takes in - which is 224x224\nfrom PIL import Image\n\n# Resize image to 214x214\ndef resize_image(image_file_path, size=(224, 224)):\n    \"\"\" Resize image to fit within the given size (224, 224) \"\"\"\n    with Image.open(image_file_path) as img:\n        img.thumbnail(size, Image.LANCZOS)\n        return img\nWith these two helper functions, we can now write our main embedding function which does a POST request.\nimport json\nimport requests\n\ndef get_embeddings(image_file_path):\n    headers = {\n        'Content-Type': 'application/json',\n        'Authorization': f'Bearer {JINA_API_KEY}'\n    }\n\n    resized_image = resize_image(image_file_path)\n\n    base64_image = image_to_base64(resized_image)\n    \n    data = {\n        'input': [{\"bytes\": base64_image}],\n        'model': 'jina-clip-v1',\n        'encoding_type': 'float'\n        }\n\n    response = requests.post(JINA_ENDPOINT,\n                             headers=headers,\n                             json=data)\n    \n    results = json.loads(response.text)\n\n    return results[\"data\"][0][\"embedding\"]\n\n\nStep 3: Loading the embeddings into the vector database\nWhat’s nice about LanceDB is that we can use Pydantic to programmatically define how we want to store our data. For the Vector type, we set it to the length 768 and that is the resulting dimensionality of the embedding vectors from jina-clip-v1.\nfrom lancedb.pydantic import Vector, LanceModel\n\n# Create a schema for the table\nclass Content(LanceModel):\n    file_id: int\n    file_name: str\n    vector: Vector(768)\n\n# Create a table called `demo` based on the above schema\ntbl = db.create_table(\"demo\", schema=Content)\n\n\nStep 4: Loading the embeddings into the vector database\nNow that we can generate our embeddings and load it into the vector database.\nFor the code below, assume list_of_files is a list of file paths of the images we want to embed.\n# Loop through each file in the list\nfor index, file_name in enumerate(LIST_OF_FILES):\n\n    # Create the embeddings\n    image_embedding = get_embeddings(file_name)\n\n    # Store this as a list of dictionaries to load into the vector database\n    img_data_to_add = [\n        {\n            \"file_id\": index\n            \"vector\": image_embedding,\n            \"file_name\": f\"{file_name}\",\n        }\n    ]\n\n    # Add to the db\n    tbl.add(img_data_to_add)\n\n\nFull code\n# Load packages\nimport base64\nimport json\nimport os\nfrom io import BytesIO\n\nimport lancedb\nimport requests\nfrom lancedb.pydantic import Vector, LanceModel\nfrom PIL import Image\n\n# Load secrets from environment variables\nLANCEDB_API_KEY = os.getenv(\"LANCEDB_API_KEY\")\nLANCEDB_URI = os.getenv(\"LANCEDB_URI\")\nJINA_API_KEY = os.getenv(\"JINA_API_KEY\")\nLIST_OF_FILES = [\"path/to/file1.png\", \"path/to/file2.png\"] # list of file names\n\n# Encode an image to base 64\ndef image_to_base64(image):\n    buffered = BytesIO()\n    image.save(buffered, format=\"PNG\")\n    return base64.b64encode(buffered.getvalue()).decode('utf-8')\n\n# Resize image to 214x214\ndef resize_image(image_file_path, size=(214, 214)):\n    \"\"\" Resize image to fit within the given size (214, 214) \"\"\"\n    with Image.open(image_file_path) as img:\n        img.thumbnail(size, Image.LANCZOS)\n        return img\n\n# Get embeddings\ndef get_embeddings(image_file_path):\n    headers = {\n        'Content-Type': 'application/json',\n        'Authorization': f'Bearer {JINA_API_KEY}'\n    }\n\n    resized_image = resize_image(image_file_path)\n\n    base64_image = image_to_base64(resized_image)\n    \n    data = {\n        'input': [{\"bytes\": base64_image}],\n        'model': 'jina-clip-v1',\n        'encoding_type': 'float'\n        }\n\n    response = requests.post(\"https://api.jina.ai/v1/embeddings\",\n                             headers=headers,\n                             json=data)\n    \n    results = json.loads(response.text)\n\n    return results[\"data\"][0][\"embedding\"]\n\n\n# Connect to the database\ndb = lancedb.connect(\n        uri=LANCEDB_URI,\n        api_key=LANCEDB_API_KEY,\n        region=\"us-east-1\"\n    )\n\n\n# Create a schema for the table\nclass Content(LanceModel):\n    file_id: int\n    file_name: str\n    vector: Vector(768)\n\n# Create a table called `demo` based on the above schema\ntbl = db.create_table(\"demo\", schema=Content)\n\n# Loop through list of file names to generate embedding and add to db\nfor index, file_name in enumerate(LIST_OF_FILES):\n    \n    # Create the embeddings\n    image_embedding = get_embeddings(file_name)\n\n    # Store this as a list of dictionaries to load into the vector database\n    img_data_to_add = [\n        {\n            \"file_id\": index,\n            \"vector\": image_embedding,\n            \"file_name\": f\"{file_name}\",\n        }\n    ]\n\n    # Add to the db\n    tbl.add(img_data_to_add)"
  },
  {
    "objectID": "posts/finetuning-tinyllama-axolotl-beginner/index.html",
    "href": "posts/finetuning-tinyllama-axolotl-beginner/index.html",
    "title": "Fine-tuning TinyLlama with Axolotl and JarvisLab",
    "section": "",
    "text": "I recently signed up for the LLM Fine-Tuning course/conference. Prior to this, most of my fine-tuning experience was either through OpenAI’s fine-tuning API for GPT 3.5, or through the starter scripts in the MLX Examples repo.\nOne tool introduced I learnt in the course was Axolotl. Basically, with Axolotl, I can easily fine-tune LLMs through the CLI based on configurations defined in a .yml file. Within the Axolotl GitHub repo, there are many helpful example .yml files one can easily re-use.\nTo run Axolotl, I’m using Jarvis Labs - a GPU cloud provider - that was also one of the generous GPU credits sponsors for the course. What I appreciated was how I could easily launch a template with Jupyter Notebook instance with the Axolot repo already cloned and other dependencies installed. For more detailed steps on getting started with Axolotl through Jarvis Labs, you can refer to this post by Andresckamilo which I also referred to.\nTo get my hands dirty and apply the above, I did up this toy example where I fine-tuned TinyLlama to generate David Attenborough style narration. The final model can be found here on HuggingFace, which you can also try.\nThe aim of this blog post is to document my learning process doing the above. Overall, the process took about 1 hour to prepare the data and fine-tune the model, though the actual fine-tuning took about 15 minutes. As such, this post also does lean more towards the data generation and preparation steps. Additionally, in terms of cost, the fine-tuning alone took slightly less than 1 USD.\n\n\nStep 1: Generating the synthetic data\nWe begin by generating the conversation pairs. For this example, I used OpenAI’s models, specifically through the Batch API. This helps to reduce cost.\nFor the Batch API, we need to provide a .jsonl file where each line is the POST request to OpenAI’s chat completion endpoint. Here is a sample of what that looks like:\n{\"custom_id\": \"0\", \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": \"gpt-4-turbo-2024-04-09\", \"messages\": [{\"role\": \"system\", \"content\": \"Imagine you are David Attenborough. I will give you an activity, and you will give a ~300 word narration for a documentary.\"}, {\"role\": \"user\", \"content\": \"A young girl planting flowers in her backyard garden.\"}], \"temperature\": 1, \"max_tokens\": 500}}\nWe begin by importing the relevant packages:\nimport json\nimport random\nimport asyncio\nfrom openai import AsyncOpenAI\nfrom tqdm import tqdm\nFor this example, we need a persona and activity for which we’ll generate the David Attenborough style narration.\nIn the code block below, we define a list of possible personas in PERSONAS, and use GPT-3.5 to generate a random activity for each persona. We set a higher temperature to allow for more creativity, and 0.6 was chosen arbitrarily. Note that we are using the async client and functions, which will be relevant in the next code block.\nFor your example, feel free to modify this seeding method for your use case.\nPERSONAS = [\"young kid\", \"young girl\", \"young boy\", \"teenager\", \"middle-aged woman\", \"middle-aged man\", \"mother\", \"father\", \"grandmother\", \"grandfather\"]\n\nclient = AsyncOpenAI()\n\nasync def fetch_activity(session, persona):\n    response = await session.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[{\n            \"role\": \"user\",\n            \"content\": f\"Generate one random activity done by a {persona}. For example, `a young boy building a sand castle`, or `a lady in the grocery store`. Be creative.\"\n        }],\n        temperature=0.6,\n        max_tokens=30\n    )\n    return response.choices[0].message.content\nWe then define the gather_activities function to make 20 concurrent GPT-3.5 API calls to generate the random activity.\nasync def gather_activities(number_rows):\n    async with client as session:\n        tasks = [fetch_activity(session, random.choice(PERSONAS)) for _ in range(number_rows)]\n        activities = []\n        for i in range(0, number_rows, 20):  # Process in chunks of 20\n            chunk = tasks[i:i+20]\n            activities.extend(await asyncio.gather(*chunk))\n        return activities\nLastly, we have the main function that creates the .jsonl file. It takes two inputs: the name of the .jsonl file you want to export and the number of rows, which is the number of requests for OpenAI’s batch API\nNote how:\n\nasyncio.run is used to execute the async functions.\ntqdm is used to track the JSONL generation.\nFor each line in the .jsonl file, we have the main API call to GPT-4 Turbo, and the system prompt is defined below.\nFor increased variation, we set the temperature to 1, which was again an arbitrary figure.\n\ndef generate_jsonl(filename, number_rows):\n    \"\"\"\n    Generate a JSONL file with the specified filename\n    \"\"\"\n    activities = asyncio.run(gather_activities(number_rows))\n\n    # Write jsonl file\n    with open(filename, 'w') as file:\n        for index in tqdm(range(0, number_rows), desc=\"Generating JSONL File\"):\n            activity = activities[index]\n            request = {\n                \"custom_id\": str(index),\n                \"method\": \"POST\",\n                \"url\": \"/v1/chat/completions\",\n                \"body\": {\n                    \"model\": \"gpt-4-turbo-2024-04-09\",\n                    \"messages\": [{\n                        \"role\": \"system\",\n                        \"content\": \"Imagine you are David Attenborough. I will give you an activity, and you will give a ~300 word narration for a documentary.\"\n                    },\n                    {\n                        \"role\": \"user\",\n                        \"content\": f\"{activity}\"\n                    }],\n                    \"temperature\": 1,\n                    \"max_tokens\": 500,\n                }\n            }\n            file.write(json.dumps(request) + '\\n')\nWe then execute the .jsonl creation. It’s worth stressing that this created david_attenborough.jsonl file is NOT the fine-tuning dataset. Instead, it’s a series of API calls that OpenAI will execute in batch, and the corresponding output of that can be formatted into our fine-tuning dataset.\n# Specify the filename and number of rows\ngenerate_jsonl('batch_api_input.jsonl', 500)\nIn this example, I used OpenAI’s developer portal GUI to upload the .jsonl file, although this can also be done programmatically. After about 3 minutes, the 500 API calls were successfully executed\n\n\nStep 2: Preparing the fine-tuning dataset\nThe next step is to prepare the fine-tuning dataset for Axolotl.\nThe output from OpenAI’s batch API is also a .jsonl file with one line per successfully completed API call. Here is what the first line of my output looked like. Note that the original prompt is not included, so I needed to merge it back.\n{\"id\": \"batch_req_HPuUiqtanStr3Bww9ydozYAr\", \"custom_id\": \"0\", \"response\": {\"status_code\": 200, \"request_id\": \"b516cd0cd1879aaf525077b91ae5b816\", \"body\": {\"id\": \"chatcmpl-9T0yV3wVCPqREUWzNoUA51psLqYY7\", \"object\": \"chat.completion\", \"created\": 1716702287, \"model\": \"gpt-4-turbo-2024-04-09\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"In the bustling heart of an urban sprawl, where the concrete jungle often dwarfs splashes of green, a small, vibrant oasis emerges in the backyard of a suburban home. Here, amidst the buzz of daily life, we discover a young girl engaged in a ritual as timeless as nature itself: the planting of flowers.\\n\\nHer small hands delve into the earth, each movement thoughtful, as if she understands the profound dialogue between human and soil. This garden, her canvas, waits patiently, offering up its nutrients and space, eager to cradle new life. With a gentle touch, she places each seedling into its own nook, crafting a mosaic of potential blooms that could transform this patch into a riot of color and fragrance.\\n\\nAs she works, her focus is palpable. She is an artist, and the earth beneath her is both medium and collaborator. Each flower, carefully selected for its hue, texture, and the role it will play in the garden\\u2019s grand symphony, is nurtured into its place. Marigolds, with their cheerful orange faces, promise a fireworks of blossoms, a natural pest repellent safeguarding her nascent creation. Lavender, with its soothing aroma, whispers of calm afternoons and the hum of busy bees.\\n\\nThis activity is more than mere horticulture; it's a lesson in patience, responsibility, and the interconnectedness of life. Our young gardener learns to foster growth, to respect cycles of life, and perhaps most poignantly, to deal with the inevitable losses that gardening, like life itself, occasionally demands.\\n\\nThe sun begins its descent, casting long shadows over freshly turned soil, transforming the ordinary into the magical. As the day ends, the garden is small still, its potential just beginning to bud. Yet, in the heart of this young botanist, dreams of spring bloom wildly\\u2014a vivid, hopeful imagining of what is yet to unfold. In her hands lies not just the beauty of flowers, but the stewardship of nature itself, one backyard at a time.\"}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"prompt_tokens\": 49, \"completion_tokens\": 413, \"total_tokens\": 462}, \"system_fingerprint\": \"fp_f17df3916f\"}}, \"error\": null}\nWe then load and extract the assistant’s content from the output .jsonl file:\ndef load_and_extract_chat_completion(jsonl_file_path):\n    contents = []\n    \n    with open(jsonl_file_path, 'r') as file:\n        for line in file:\n            data = json.loads(line)\n            assistant_content = data.get(\"response\", {}).get(\"body\", {}).get(\"choices\", [])[0].get(\"message\", {}).get(\"content\", \"\")\n            if assistant_content:\n                contents.append(assistant_content)\n    \n    return contents\n\nanswers = load_and_extract_chat_completion('batch_api_output.jsonl')\nWe also load and extract the original prompts.\ndef load_and_extract_prompt(jsonl_file_path):\n    user_contents = []\n    \n    with open(jsonl_file_path, 'r') as file:\n        for line in file:\n            data = json.loads(line)\n            messages = data.get(\"body\", {}).get(\"messages\", [])\n            if len(messages) &gt; 1:\n                user_content = messages[1].get(\"content\", \"\")\n                if user_content:\n                    user_contents.append(user_content)\n    \n    return user_contents\n\nprompt = load_and_extract_prompt(\"batch_api_input.jsonl\")\nWe’ve now stored the original prompt and the response in two separate Python lists. We will now combine them into the sharegpt format\n# Create fine-tuning dataset\nwith open('david_attenborough_conversations.jsonl', 'w') as outfile:\n    for i in range(len(prompt)):\n        example = {\n            \"conversations\": [\n                {\"role\": \"human\", \"value\": f\"Write a David Attenborough style commentary based on the following prompt: {prompt[i]}\"},\n                {\"role\": \"assistant\", \"value\": f\"{answers[i]}\"}\n            ]\n        }\n        json.dump(example, outfile)\n        outfile.write('\\n')\n\n\nStep 3: Uploading the dataset to HuggingFace\nThe next step is to upload it to HuggingFace - which we can then pull via Axolotl when running the fine-tuning job.\nYou can find the final dataset here on HuggingFace.\n\n\nStep 4: Running Axolotl through JarvisLab\nSo now that we’ve prepared our data - we can begin the fine-tuning proper.\nIronically, this step took the least amount of time mainly because data preparation (rightfully) takes more time, but also due to the convenient abstractions provided by Axolotl.\nI modified the dataset keys for the lora.yml for TinyLlama that was located at examples/tiny-llama/lora.yml.\ndatasets:\n  - path: cyzgab/david_attenborough_style_narration\n    type: sharegpt\ndataset_prepared_path:\nval_set_size: 0.05\noutput_dir: ./outputs/lora-out\nThen the next step is to begin the training job which was on about 475 training examples and 4 epochs. This took about 15 minutes to complete.\n# start training job\naccelerate launch -m axolotl.cli.train examples/tiny-llama/lora.yml\nOnce the training job is done, I ran the following command to launch a Gradio interface to interact with the model.\n# gradio \naccelerate launch -m axolotl.cli.inference examples/tiny-llama/lora.yml --lora_model_dir=\"./outputs/lora-out\" --gradio\nLastly, I ran the following command to fuse the lora with the original weights, which I then uploaded to HuggingFace.\n# fuse model\npython3 -m axolotl.cli.merge_lora examples/tiny-llama/lora.yml --lora_model_dir=\"./outputs/lora-out\"\n\n\nConclusion\nOverall, this was a fun gentle introduction to fine-tuning an open source model. Utilizing synthetic data allowed me to generate unique content tailored to the use case. This process was further streamlined by the powerful abstractions provided by tools like Axolotl and the Batch API, which made it quick and efficient"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hello 👋",
    "section": "",
    "text": "I’m Data Scientist at GovTech, where my current work centers on: (1) MLOps, (2) prototyping of LLM solutions, (3) and Responsible AI. Additionally, I co-organise AI Wednesdays, a weekly gathering of AI practitioners, and other community hackathons and demo sessions.\nBefore this role, I was a policy analyst at the Ministry of Health, where I focused on financing policies and analytics for COVID-19.\nI hold a BSc in Economics from LSE and a Master’s in Business Analytics from MIT.\nIn my free time, I practise pilates, exploring the outdoors, and enjoy a good beer."
  }
]