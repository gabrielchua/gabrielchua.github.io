[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Notes for ‚ÄúChameleon: Mixed-Modal Early-Fusion Foundation Models‚Äù\n\n\n\n\n\n\npersonal notes\n\n\nllm\n\n\n\n\n\n\n\n\n\nJun 5, 2024\n\n\nGabriel Chua\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/chameleon-meta/index.html",
    "href": "posts/chameleon-meta/index.html",
    "title": "Notes for ‚ÄúChameleon: Mixed-Modal Early-Fusion Foundation Models‚Äù",
    "section": "",
    "text": "Here is the link to the original paper. These notes were prepared for my LLM Asia Paper Club sharing. Any feedback or areas for improvement would be most appreciated at cyzgab[at]gmail.com.\n\n\nKey Points\n\nEnd-to-end multimodal tokens (i.e.¬†no modality-specific encoder or decoder)\nNovelties in architectural innovations and training techniques to address computational challenges:\n\nquery-key normalization\nrevised placement of layer norms\n\nPre-training these models require large datasets and computation.\n\nDataset of 4.4T (2.9T is text only, 1.5T is text-to-image, and 400B is text-and-image-interleaved)\n7B and 34B trained on 856K and 4.2M GPU hours respectively\n\nPerformance:\n\nOn Visual Q&A, outperforms Flamingo, Llava-1.5\nOn text-only benchmarks, still competitive with Mixtral 8x7B and Gemini-Pro\nOn human pairwise comparisons, beats Gemini-Pro and GPT-4V\n\n\n\n\n\n\nImage 1: Conceptual summary of multimodal training and generation from the paper\n\n\n\n\n\n\nImage 2: Example generation from the paper\n\n\n\n\n\nLate vs Early Fusion\n\nA useful reference for me was this literature review by Wadekar et.al\n\nLate fusion Done at the internal layers of the model (e.g.¬†OpenFlamingo, LLaMA-Adapter-V2)\n\n\n\n\nImage 3: Simplified architectural summary for late fusion - taken from Wadekar et.al\n\n\n\nEarly fusion Done at the input stage (e.g.¬†LLaVA, Unified-IO-2, Chameleon, Gemini)\n\n\n\n\nImage 4: Simplified architectural summary for non-tokenised early fusion - taken from Wadekar et.al\n\n\n\n\n\n\nImage 5: Simplified architectural summary for tokenised early fusion - taken from Wadekar et.al\n\n\n\n\n\nTokeniser\n\nFor images: trained a new image tokenizer based on Gafni et.al (2022) which encodes a 512 √ó 512 image into 1024 discrete tokens from a codebook of size 8192\n\n\nBased on OAI‚Äôs pricing page (as of 5 June 2024), one image is ~170 tokens in GPT-4o.\n\n\nFor text: BPE tokenizer over a subset of training data, with a vocabulary size of 65K\n\n\n\nEnsuring Stability of Pre-Training\n\n‚ÄúWe found that the standard LLaMa architecture showed complex divergences due to slow norm growth in the mid-to-late stages of training. We narrowed down the cause of the divergence to the softmax operation being problematic when training with multiple modalities of significantly varying entropy due to the translation invariant property of softmax (i.e., softmax(z) = softmax(z+c)). Because we share all weights of the model across modalities, each modality will try to ‚Äúcompete‚Äù with the other by increasing its norms slightly; while not problematic at the beginning of training, it manifests in divergences once we get outside the effective representation range of bf16‚Ä¶ In a unimodal setting, this problem has also been named the logit drift problem.‚Äù (Page 6)\n\n\nQuery-key normalisation: applying layer norm to the query and key vectors within the attention.\nRevised placement of layer norms for 34B model\nIntroducing z-loss regularisation\n\n\n\n\n\nImage 6: Training plots\n\n\n\nDropout was initially introduced after the attention and feed forward layer for the 7B model, though subsequently found to be not necessary. For the 34B model, dropout was not sufficient (nor necessary).\n\n\nSummary of Pre-Training\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nParams\nContext Length\nGQA\nTokens\nLR\nEpochs\nDropout\nZloss\nQknorm\n\n\n\n\nLLMa-1\n7B\n2k\n‚úó\n1.0T\n3.0 √ó 10^-4\n1.0\n0.0\n0.0\n‚úó\n\n\n\n33B\n2k\n‚úó\n1.4T\n1.5 √ó 10^-4\n1.0\n0.0\n0.0\n‚úó\n\n\nLLMa-2\n7B\n4k\n‚úó\n2.0T\n3.0 √ó 10^-4\n1.0\n0.0\n0.0\n‚úó\n\n\n\n34B\n4k\n‚úì\n2.0T\n1.5 √ó 10^-4\n1.0\n0.0\n0.0\n‚úó\n\n\nChameleon\n7B\n4k\n‚úó\n4.4T\n1.0 √ó 10^-4\n2.1\n0.1\n10^-5\n‚úì\n\n\n\n34B\n4k\n‚úì\n4.4T\n1.0 √ó 10^-4\n2.1\n0.0\n10^-5\n‚úì\n\n\n\nTaken from Table 1 of the paper\n\n\nChallenges Associated with Inference\n\nWhen decoding, we need to check whether it is a text or image token\nMasking tokens from other modalities when exclusively generating for a particular modality (e.g.¬†no text tokens when doing image-only generation)\nToken-based image generation is a fixed-sized block\n\n\n\nSFT/Alignment\nSupervised fine-tuning dataset covered the following categories:\n\ntext\ncode\nvisual chat\nimage generation\ninterleaved text/image generation\nsafety (e.g.¬†‚ÄúI can‚Äôt help with that‚Äù)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hello üëã",
    "section": "",
    "text": "I am a Data Scientist at GovTech where my current work focuses on MLOps and prototyping LLM applications for labour market applications.\nPreviously, I served as a policy analyst at the Ministry of Health reviewing primary care financing policies (e.g.¬†subsidies, capitation funding for population health) and data analytics for COVID-19. My proudest achievements include contributing to the data pipelines for the nightly COVID-19 press releases, and co-authoring a White Paper that was debated in Parliament.\nI gravitate towards new ideas and am curious about the intersection of different disciplines. With a background in the social sciences, my career evolved towards applied machine learning. I hold a BSc in Economics from LSE and a Master‚Äôs in Business Analytics from MIT.\nOutside of work, I enjoy pilates, a good beer, and hiking."
  }
]